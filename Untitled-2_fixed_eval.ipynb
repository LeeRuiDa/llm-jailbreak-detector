{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "650ee5dc-5cc5-403f-86ef-b544d3cf178e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "import hashlib\n",
        "import platform\n",
        "import random\n",
        "from datetime import datetime\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "cuda_ok = False\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        x = torch.randn(8, 8, device=\"cuda\")\n",
        "        y = (x @ x).sum().item()\n",
        "        cuda_ok = True\n",
        "    except Exception as e:\n",
        "        cuda_ok = False\n",
        "        print(\"CUDA reported available but is unusable; falling back to CPU. Reason:\", repr(e))\n",
        "        print(\"Install a PyTorch build that supports your GPU's compute capability (sm_120) or run on Colab.\")\n",
        "\n",
        "device = \"cuda\" if cuda_ok else \"cpu\"\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be54c79-40f6-42f5-9d03-632c8aaba11f",
      "metadata": {},
      "outputs": [],
      "source": [
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    import subprocess, sys as _sys\n",
        "    subprocess.check_call([_sys.executable, \"-m\", \"pip\", \"install\", \"-U\",\n",
        "                           \"transformers\", \"peft\", \"accelerate\", \"datasets\",\n",
        "                           \"bitsandbytes\", \"scikit-learn\", \"pandas\", \"tqdm\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(\"Seed:\", SEED)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bffad0-c12f-4bf8-a6e9-23a8c0332666",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ROOT = os.getenv(\"PROJECT_ROOT\", os.getcwd())\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "\n",
        "DATASET_NAME = \"v1\"\n",
        "BASELINE_VERSION = \"v0.2-week3\"\n",
        "TRAIN_PATH = os.getenv(\"TRAIN_PATH\", os.path.join(PROJECT_ROOT, \"data\", \"v1\", \"train.jsonl\"))\n",
        "VAL_PATH = os.getenv(\"VAL_PATH\", os.path.join(PROJECT_ROOT, \"data\", \"v1\", \"val.jsonl\"))\n",
        "TEST_MAIN_PATH = os.getenv(\"TEST_MAIN_PATH\", os.path.join(PROJECT_ROOT, \"data\", \"v1\", \"test_main.jsonl\"))\n",
        "TEST_JBB_PATH = os.getenv(\"TEST_JBB_PATH\", os.path.join(PROJECT_ROOT, \"data\", \"v1_holdout\", \"test_jbb.jsonl\"))\n",
        "USE_UNICODE = False\n",
        "ATTACK_CLASS_INDEX = 0  # Set based on sanity checks.\n",
        "RUNS_DIR = (\n",
        "    os.path.join(\"/content/drive/MyDrive\", \"capstone_runs\")\n",
        "    if IN_COLAB\n",
        "    else os.path.join(PROJECT_ROOT, \"runs\")\n",
        ")\n",
        "unicode_tag = \"u1\" if USE_UNICODE else \"u0\"\n",
        "RUN_NAME = os.getenv(\"RUN_NAME\", \"\").strip()\n",
        "default_name = f\"lora_{DATASET_NAME}_{unicode_tag}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "base_name = RUN_NAME if RUN_NAME else default_name\n",
        "EXPERIMENT_NAME = base_name\n",
        "OUT_DIR = os.path.join(RUNS_DIR, EXPERIMENT_NAME)\n",
        "if os.path.exists(OUT_DIR):\n",
        "    suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    EXPERIMENT_NAME = f\"{base_name}_{suffix}\"\n",
        "    OUT_DIR = os.path.join(RUNS_DIR, EXPERIMENT_NAME)\n",
        "    counter = 1\n",
        "    while os.path.exists(OUT_DIR):\n",
        "        EXPERIMENT_NAME = f\"{base_name}_{suffix}_{counter}\"\n",
        "        OUT_DIR = os.path.join(RUNS_DIR, EXPERIMENT_NAME)\n",
        "        counter += 1\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n",
        "print(\"TRAIN_PATH:\", TRAIN_PATH)\n",
        "print(\"VAL_PATH:\", VAL_PATH)\n",
        "print(\"TEST_MAIN_PATH:\", TEST_MAIN_PATH)\n",
        "print(\"TEST_JBB_PATH:\", TEST_JBB_PATH)\n",
        "for path in [TRAIN_PATH, VAL_PATH, TEST_MAIN_PATH, TEST_JBB_PATH]:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing dataset file: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10895af-030c-4a24-b41f-7b066701acaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data.io import load_examples\n",
        "from src.preprocess.unicode import normalize_text\n",
        "\n",
        "def load_records(path: str, use_unicode: bool):\n",
        "    examples = load_examples(path)\n",
        "    rows = []\n",
        "    for ex in examples:\n",
        "        text = ex.text\n",
        "        if use_unicode:\n",
        "            text = normalize_text(text)\n",
        "        rows.append(\n",
        "            {\n",
        "                \"id\": ex.id,\n",
        "                \"text\": text,\n",
        "                \"label\": ex.label,\n",
        "                \"attack_type\": ex.attack_type,\n",
        "                \"meta\": ex.meta,\n",
        "            }\n",
        "        )\n",
        "    return rows\n",
        "\n",
        "train_records = load_records(TRAIN_PATH, USE_UNICODE)\n",
        "val_records = load_records(VAL_PATH, USE_UNICODE)\n",
        "test_main_records = load_records(TEST_MAIN_PATH, USE_UNICODE)\n",
        "test_jbb_records = load_records(TEST_JBB_PATH, USE_UNICODE)\n",
        "\n",
        "print(\"Loaded:\", len(train_records), len(val_records), len(test_main_records), len(test_jbb_records))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f73a078-858f-4f71-8d3f-ced19ea731d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(\n",
        "    \"Train/Val/Test_Main/Test_JBB:\",\n",
        "    len(train_records),\n",
        "    len(val_records),\n",
        "    len(test_main_records),\n",
        "    len(test_jbb_records),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d692395f-a5c0-44a9-aaaf-001f32299075",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, rows, tokenizer, max_length):\n",
        "        self.rows = rows\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.rows[idx]\n",
        "        enc = self.tokenizer(\n",
        "            row[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "        )\n",
        "        enc[\"labels\"] = row[\"label\"]\n",
        "        enc[\"id\"] = row[\"id\"]\n",
        "        return enc\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "TENSOR_KEYS = {\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\", \"label\"}\n",
        "\n",
        "def collate_with_extras(features):\n",
        "    extras = {}\n",
        "    for k in list(features[0].keys()):\n",
        "        if k not in TENSOR_KEYS:\n",
        "            extras[k] = [f.pop(k) for f in features]\n",
        "    batch = data_collator(features)\n",
        "    if \"label\" in batch and \"labels\" not in batch:\n",
        "        batch[\"labels\"] = batch.pop(\"label\")\n",
        "    batch.update(extras)\n",
        "    return batch\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_ds = TextDataset(train_records, tokenizer, MAX_LENGTH)\n",
        "val_ds = TextDataset(val_records, tokenizer, MAX_LENGTH)\n",
        "test_main_ds = TextDataset(test_main_records, tokenizer, MAX_LENGTH)\n",
        "test_jbb_ds = TextDataset(test_jbb_records, tokenizer, MAX_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_with_extras)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_extras)\n",
        "test_main_loader = DataLoader(test_main_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_extras)\n",
        "test_jbb_loader = DataLoader(test_jbb_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_extras)\n",
        "\n",
        "test_ds = test_main_ds\n",
        "test_loader = test_main_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b021f50-a37a-4570-941e-1138e4b62a2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"collate_fn:\", train_loader.collate_fn)\n",
        "batch = next(iter(train_loader))\n",
        "print(\"keys:\", batch.keys())\n",
        "print(\"id type:\", type(batch[\"id\"]), \"first id:\", batch[\"id\"][0])\n",
        "labels = batch[\"labels\"]\n",
        "print(\"labels dtype/shape:\", labels.dtype, labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f4d7082-80f8-49dd-bc7a-632ebeb139b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_lin\", \"v_lin\"],\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1326308-4373-4fc7-9b06-4a8d0536bcb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "EPOCHS = 3\n",
        "LR = 2e-4\n",
        "GRAD_ACCUM_STEPS = 2\n",
        "WARMUP_RATIO = 0.06\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "num_update_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
        "total_steps = num_update_steps_per_epoch * EPOCHS\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "use_bf16 = device == \"cuda\" and torch.cuda.is_bf16_supported()\n",
        "autocast_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device == \"cuda\" and not use_bf16))\n",
        "\n",
        "def save_checkpoint(path, model, optimizer, scheduler, scaler, epoch, global_step):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    torch.save(\n",
        "        {\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scheduler_state\": scheduler.state_dict(),\n",
        "            \"scaler_state\": scaler.state_dict() if scaler is not None else None,\n",
        "            \"epoch\": epoch,\n",
        "            \"global_step\": global_step,\n",
        "        },\n",
        "        os.path.join(path, \"checkpoint.pt\"),\n",
        "    )\n",
        "\n",
        "def load_checkpoint(path, model, optimizer, scheduler, scaler):\n",
        "    ckpt = torch.load(os.path.join(path, \"checkpoint.pt\"), map_location=\"cpu\")\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "    if scaler is not None and ckpt[\"scaler_state\"] is not None:\n",
        "        scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
        "    return ckpt.get(\"epoch\", 0), ckpt.get(\"global_step\", 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ac3273-750f-461f-a27e-3ba79cccf7ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "RESUME_FROM = \"\"  # set to a checkpoint dir to resume\n",
        "start_epoch = 0\n",
        "global_step = 0\n",
        "if RESUME_FROM:\n",
        "    start_epoch, global_step = load_checkpoint(\n",
        "        RESUME_FROM, model, optimizer, scheduler, scaler\n",
        "    )\n",
        "    print(\"Resumed from\", RESUME_FROM, \"epoch\", start_epoch, \"step\", global_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb001f88-d478-4328-82c5-332a7825cd9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_idx):\n",
        "    global global_step\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    step = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader, start=1):\n",
        "        tensor_batch = {k: v.to(device, non_blocking=True) for k, v in batch.items() if torch.is_tensor(v)}\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=autocast_dtype) if device == \"cuda\" else nullcontext():\n",
        "            out = model(**tensor_batch)\n",
        "            loss = out.loss / GRAD_ACCUM_STEPS\n",
        "\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        if step % GRAD_ACCUM_STEPS == 0:\n",
        "            if scaler.is_enabled():\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            global_step += 1\n",
        "\n",
        "        running_loss += loss.item() * GRAD_ACCUM_STEPS\n",
        "\n",
        "    if step > 0 and step % GRAD_ACCUM_STEPS != 0:\n",
        "        if scaler.is_enabled():\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        global_step += 1\n",
        "\n",
        "    return running_loss / max(1, len(train_loader))\n",
        "\n",
        "def run_eval(loader):\n",
        "    \"\"\"Evaluation without autocast and without numpy conversion (bf16-safe).\"\"\"\n",
        "    model.eval()\n",
        "    all_ids, all_labels, all_scores, all_scores_alt = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch[\"id\"]          # list[str]\n",
        "            labels = batch[\"labels\"]   # tensor on CPU\n",
        "            tensor_batch = {\n",
        "                k: v.to(device, non_blocking=True)\n",
        "                for k, v in batch.items()\n",
        "                if torch.is_tensor(v) and k != \"labels\"\n",
        "            }\n",
        "            logits = model(**tensor_batch).logits\n",
        "            probs = torch.softmax(logits.float(), dim=-1)   # force fp32\n",
        "            score_attack = probs[:, ATTACK_CLASS_INDEX]\n",
        "            score_alt = probs[:, 1 - ATTACK_CLASS_INDEX]\n",
        "            all_ids.extend(ids)\n",
        "            all_labels.extend(labels.detach().cpu().tolist())\n",
        "            all_scores.extend(score_attack.detach().cpu().tolist())      # no numpy\n",
        "            all_scores_alt.extend(score_alt.detach().cpu().tolist())\n",
        "    return all_ids, all_labels, all_scores, all_scores_alt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c52279-4f84-4451-9930-cbd30a804726",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.eval.metrics import compute_metrics\n",
        "\n",
        "metrics_history = []\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    loss = train_one_epoch(epoch)\n",
        "    _, y_true, y_score, y_score_alt = run_eval(val_loader)\n",
        "    if len(set(y_true)) > 1:\n",
        "        val_auc = roc_auc_score(y_true, y_score)\n",
        "        if val_auc < 0.5:\n",
        "            alt_auc = roc_auc_score(y_true, y_score_alt)\n",
        "            raise RuntimeError(\n",
        "                f\"Score orientation wrong; check label mapping or class index. \"\n",
        "                f\"attack_class_index={ATTACK_CLASS_INDEX} auc={val_auc:.4f} auc(alt)={alt_auc:.4f}\"\n",
        "            )\n",
        "    metrics = compute_metrics(y_true, y_score, target_fpr=0.01)\n",
        "    metrics[\"epoch\"] = epoch\n",
        "    metrics[\"train_loss\"] = loss\n",
        "    metrics_history.append(metrics)\n",
        "    print(\"Epoch\", epoch, \"loss\", round(loss, 4), \"auroc\", metrics[\"auroc\"])\n",
        "\n",
        "    ckpt_dir = os.path.join(OUT_DIR, f\"ckpt_epoch_{epoch}\")\n",
        "    save_checkpoint(ckpt_dir, model, optimizer, scheduler, scaler, epoch, global_step)\n",
        "\n",
        "metrics_path = os.path.join(OUT_DIR, \"metrics.json\")\n",
        "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics_history, f, indent=2)\n",
        "print(\"Saved metrics to\", metrics_path)",
        "\n",
        "trained_dir = os.path.join(OUT_DIR, \"trained_model\")\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(trained_dir)\n",
        "tokenizer.save_pretrained(trained_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(trained_dir)\n",
        "model.to(device)\n",
        "print(\"Reloaded trained model from\", trained_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "213437ab-a860-40cc-be7f-48b7b04e1509",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import timezone\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from src.eval.metrics import tpr_at_fpr\n",
        "import csv\n",
        "\n",
        "def write_predictions(path, ids, y_true, y_score, split_name):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for ex_id, y, s in zip(ids, y_true, y_score):\n",
        "            row = {\"id\": ex_id, \"label\": int(y), \"score\": float(s), \"split\": split_name}\n",
        "            f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "def write_predictions_csv(path, ids, y_true, y_score, y_score_alt, split_name):\n",
        "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"id\", \"label\", \"score_attack\", \"score_alt\", \"split\"])\n",
        "        for ex_id, y, s, s_alt in zip(ids, y_true, y_score, y_score_alt):\n",
        "            writer.writerow([ex_id, int(y), float(s), float(s_alt), split_name])\n",
        "\n",
        "def compute_split_metrics(y_true, y_score, threshold):\n",
        "    if len(set(y_true)) > 1:\n",
        "        auroc = roc_auc_score(y_true, y_score)\n",
        "        auprc = average_precision_score(y_true, y_score)\n",
        "    else:\n",
        "        auroc = None\n",
        "        auprc = None\n",
        "    pred_attack = [s >= threshold for s in y_score]\n",
        "    pos = [y == 1 for y in y_true]\n",
        "    neg = [y == 0 for y in y_true]\n",
        "    tpr = (sum(p and y for p, y in zip(pred_attack, pos)) / sum(pos)) if sum(pos) else 0.0\n",
        "    fpr = (sum(p and y for p, y in zip(pred_attack, neg)) / sum(neg)) if sum(neg) else 0.0\n",
        "    asr = (sum((not p) and y for p, y in zip(pred_attack, pos)) / sum(pos)) if sum(pos) else 0.0\n",
        "    return {\n",
        "        \"auroc\": auroc,\n",
        "        \"auprc\": auprc,\n",
        "        \"tpr_at_fpr\": tpr,\n",
        "        \"fpr_actual\": fpr,\n",
        "        \"threshold\": float(threshold),\n",
        "        \"asr_at_threshold\": asr,\n",
        "    }\n",
        "\n",
        "# --- val calibration ---\n",
        "val_ids, val_y_true, val_y_score, _ = run_eval(val_loader)\n",
        "val_pred_path = os.path.join(OUT_DIR, \"predictions_val.jsonl\")\n",
        "write_predictions(val_pred_path, val_ids, val_y_true, val_y_score, \"val\")\n",
        "op = tpr_at_fpr(val_y_true, val_y_score, target_fpr=0.01)\n",
        "val_op_path = os.path.join(OUT_DIR, \"val_operating_point.json\")\n",
        "with open(val_op_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(op.__dict__, f, indent=2)\n",
        "val_metrics = compute_split_metrics(val_y_true, val_y_score, op.threshold)\n",
        "val_metrics[\"target_fpr\"] = 0.01\n",
        "val_metrics[\"split\"] = \"val\"\n",
        "val_metrics[\"threshold_source\"] = \"val\"\n",
        "val_metrics_path = os.path.join(OUT_DIR, \"final_metrics_val.json\")\n",
        "with open(val_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_metrics, f, indent=2)\n",
        "\n",
        "# --- test_main ---\n",
        "tm_ids, tm_y_true, tm_y_score, tm_y_score_alt = run_eval(test_main_loader)\n",
        "tm_pred_path = os.path.join(OUT_DIR, \"predictions_test_main.jsonl\")\n",
        "write_predictions(tm_pred_path, tm_ids, tm_y_true, tm_y_score, \"test_main\")\n",
        "artifacts_dir = os.path.join(PROJECT_ROOT, \"artifacts\", \"eval\")\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "tm_csv_path = os.path.join(artifacts_dir, \"predictions_test_main.csv\")\n",
        "write_predictions_csv(tm_csv_path, tm_ids, tm_y_true, tm_y_score, tm_y_score_alt, \"test_main\")\n",
        "tm_metrics = compute_split_metrics(tm_y_true, tm_y_score, op.threshold)\n",
        "tm_metrics[\"target_fpr\"] = 0.01\n",
        "tm_metrics[\"split\"] = \"test_main\"\n",
        "tm_metrics[\"threshold_source\"] = \"val\"\n",
        "tm_metrics_path = os.path.join(OUT_DIR, \"final_metrics_test_main.json\")\n",
        "with open(tm_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tm_metrics, f, indent=2)\n",
        "\n",
        "# --- test_jbb ---\n",
        "jbb_ids, jbb_y_true, jbb_y_score, jbb_y_score_alt = run_eval(test_jbb_loader)\n",
        "jbb_pred_path = os.path.join(OUT_DIR, \"predictions_test_jbb.jsonl\")\n",
        "write_predictions(jbb_pred_path, jbb_ids, jbb_y_true, jbb_y_score, \"test_jbb\")\n",
        "jbb_csv_path = os.path.join(artifacts_dir, \"predictions_test_jbb.csv\")\n",
        "write_predictions_csv(jbb_csv_path, jbb_ids, jbb_y_true, jbb_y_score, jbb_y_score_alt, \"test_jbb\")\n",
        "jbb_metrics = compute_split_metrics(jbb_y_true, jbb_y_score, op.threshold)\n",
        "jbb_metrics[\"target_fpr\"] = 0.01\n",
        "jbb_metrics[\"split\"] = \"test_jbb\"\n",
        "jbb_metrics[\"threshold_source\"] = \"val\"\n",
        "jbb_metrics_path = os.path.join(OUT_DIR, \"final_metrics_test_jbb.json\")\n",
        "with open(jbb_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(jbb_metrics, f, indent=2)\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def _get_git_commit(repo_root: str):\n",
        "    try:\n",
        "        return subprocess.check_output(\n",
        "            [\"git\", \"rev-parse\", \"HEAD\"],\n",
        "            cwd=repo_root,\n",
        "            text=True,\n",
        "            stderr=subprocess.DEVNULL,\n",
        "        ).strip()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "git_commit = _get_git_commit(PROJECT_ROOT)\n",
        "\n",
        "config = {\n",
        "    \"dataset_name\": DATASET_NAME,\n",
        "    \"detector\": \"lora\",\n",
        "    \"unicode\": USE_UNICODE,\n",
        "    \"baseline_version\": BASELINE_VERSION,\n",
        "    \"attack_class_index\": ATTACK_CLASS_INDEX,\n",
        "    \"splits\": [\"val\", \"test_main\", \"test_jbb\"],\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"val_path\": VAL_PATH,\n",
        "    \"test_main_path\": TEST_MAIN_PATH,\n",
        "    \"test_jbb_path\": TEST_JBB_PATH,\n",
        "    \"target_fpr\": 0.01,\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"seed\": SEED,\n",
        "    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"run_dir\": OUT_DIR,\n",
        "}\n",
        "if git_commit:\n",
        "    config[\"git_commit\"] = git_commit\n",
        "config_path = os.path.join(OUT_DIR, \"config.json\")\n",
        "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "model.save_pretrained(os.path.join(OUT_DIR, \"lora_adapter\"))\n",
        "tokenizer.save_pretrained(os.path.join(OUT_DIR, \"lora_adapter\"))\n",
        "\n",
        "def _fmt_mtime(path: str) -> str:\n",
        "    return datetime.fromtimestamp(os.path.getmtime(path)).isoformat()\n",
        "\n",
        "print(\"Saved val predictions to\", val_pred_path, \"mtime\", _fmt_mtime(val_pred_path))\n",
        "print(\"Saved test_main predictions to\", tm_pred_path, \"mtime\", _fmt_mtime(tm_pred_path))\n",
        "print(\"Saved test_jbb predictions to\", jbb_pred_path, \"mtime\", _fmt_mtime(jbb_pred_path))\n",
        "print(\"Saved val metrics to\", val_metrics_path, \"mtime\", _fmt_mtime(val_metrics_path))\n",
        "print(\"Saved test_main metrics to\", tm_metrics_path, \"mtime\", _fmt_mtime(tm_metrics_path))\n",
        "print(\"Saved test_jbb metrics to\", jbb_metrics_path, \"mtime\", _fmt_mtime(jbb_metrics_path))\n",
        "print(\"Saved config to\", config_path, \"mtime\", _fmt_mtime(config_path))\n",
        "\n",
        "import subprocess\n",
        "verify_script = os.path.join(PROJECT_ROOT, \"scripts\", \"verify_run_artifacts.py\")\n",
        "subprocess.check_call([\n",
        "    sys.executable,\n",
        "    verify_script,\n",
        "    \"--run_dir\",\n",
        "    OUT_DIR,\n",
        "    \"--predictions\",\n",
        "    val_pred_path,\n",
        "])"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": 3
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}