# LLM Jailbreak Detector

This project aims to build a low false-positive rate detector for jailbreak and prompt-injection attacks on LLM apps. Inspired by recent research and public toolkits (e.g., PINT, JailbreakBench, PromptShield).

## Structure
- data/
- src/
  - preprocess.py
  - rulesbaseline.py
- reports/
- README.md

## Quick Start
Clone, then see src/preprocess.py and src/rulesbaseline.py for initial code.