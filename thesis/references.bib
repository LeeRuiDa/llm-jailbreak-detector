@article{devlin2019bert,
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2019}
}

@article{liu2019roberta,
  title   = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author  = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal = {arXiv preprint arXiv:1907.11692},
  year    = {2019}
}

@article{he2021deberta,
  title   = {DeBERTa: Decoding-Enhanced BERT with Disentangled Attention},
  author  = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2006.03654},
  year    = {2021}
}

@article{brown2020gpt3,
  title   = {Language Models are Few-Shot Learners},
  author  = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {arXiv preprint arXiv:2005.14165},
  year    = {2020}
}

@article{goodfellow2015adversarial,
  title   = {Explaining and Harnessing Adversarial Examples},
  author  = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  journal = {arXiv preprint arXiv:1412.6572},
  year    = {2015}
}

@article{madry2018towards,
  title   = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author  = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal = {arXiv preprint arXiv:1706.06083},
  year    = {2018}
}

@article{gehman2020realtotoxicity,
  title   = {RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author  = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
  journal = {arXiv preprint arXiv:2009.11462},
  year    = {2020}
}

@article{bai2022constitutional,
  title   = {Constitutional AI: Harmlessness from AI Feedback},
  author  = {Bai, Yuntao and others},
  journal = {arXiv preprint arXiv:2212.08073},
  year    = {2022}
}

@article{perez2022redteaming,
  title   = {Red Teaming Language Models with Language Models},
  author  = {Perez, Ethan and others},
  journal = {arXiv preprint arXiv:2202.03286},
  year    = {2022}
}

@misc{unicode_uax15,
  title        = {Unicode Standard Annex \#15: Unicode Normalization Forms},
  author       = {The Unicode Consortium},
  howpublished = {\url{https://unicode.org/reports/tr15/}}
}

@misc{unicode_uts39,
  title        = {Unicode Technical Standard \#39: Unicode Security Mechanisms},
  author       = {The Unicode Consortium},
  howpublished = {\url{https://unicode.org/reports/tr39/}}
}

@misc{todo_prompt_injection_survey_2024,
  note = {TODO: fill}
}

@misc{todo_jailbreakbench_2023,
  note = {TODO: fill}
}

@misc{todo_prompt_injection_attacks_2024,
  note = {TODO: fill}
}

@misc{todo_safety_classifier_eval_2023,
  note = {TODO: fill}
}

@misc{todo_unicode_security_paper,
  note = {TODO: fill}
}

@misc{todo_redteaming_report_2023,
  note = {TODO: fill}
}

@misc{todo_adversarial_llm_attacks_2023,
  note = {TODO: fill}
}

@misc{todo_prompt_leakage_2023,
  note = {TODO: fill}
}

@misc{todo_harmful_content_detection_2024,
  note = {TODO: fill}
}

@misc{todo_guardrails_survey_2024,
  note = {TODO: fill}
}
